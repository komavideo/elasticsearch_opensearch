{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- [Instructions](https://opensearch-project.github.io/opensearch-py-ml/examples/demo_deploy_cliptextmodel.html) on documentation missing \n",
    "- Tried many ways but I believe mlcommons framework doesn't support CLIP \n",
    "- As per [this](https://opensearch.org/docs/latest/ml-commons-plugin/custom-local-models/) only local text, local sparse, cross-encoder and question-answering models are allowed for local models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPTextModel\n",
    "import torch\n",
    "\n",
    "import opensearch_py_ml as oml\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unverified HTTPS request\")\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"TracerWarning: torch.tensor\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"using SSL with verify_certs=False is insecure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTER_URL = 'https://192.18.0.111:9200'\n",
    "CLUSTER_URL = {'host': '192.168.0.111', 'port': 9200}\n",
    "\n",
    "def get_os_client(cluster_url = CLUSTER_URL,\n",
    "                  username='admin',\n",
    "                  password='Developer@123'):\n",
    "    '''\n",
    "    Get OpenSearch client\n",
    "    :param cluster_url: cluster URL like https://ml-te-netwo-1s12ba42br23v-ff1736fa7db98ff2.elb.us-west-2.amazonaws.com:443\n",
    "    :return: OpenSearch client\n",
    "    '''\n",
    "    client = OpenSearch(\n",
    "        hosts=[cluster_url], #[cluster_url], # {'host': '192.168.0.111', 'port': 9200}\n",
    "        http_auth=(username, password),\n",
    "        verify_certs=False,\n",
    "        ssl_assert_hostname = False,\n",
    "        ssl_show_warn = False,\n",
    "        use_ssl=True\n",
    "    )\n",
    "    return client\n",
    "\n",
    "client = get_os_client()\n",
    "\n",
    "# Update cluster settings to allow local model registration\n",
    "client.cluster.put_settings(body={\n",
    "    \"persistent\": {\n",
    "        \"plugins\": {\n",
    "            \"ml_commons\": {\n",
    "                \"allow_registering_model_via_url\": \"true\",\n",
    "                \"allow_registering_model_via_local_file\": \"true\",\n",
    "                \"only_run_on_ml_node\": \"false\",\n",
    "                \"model_access_control_enabled\": \"true\",\n",
    "                \"native_memory_threshold\": \"99\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "# Connect to ml_common client with OpenSearch client\n",
    "ml_client = MLCommonClient(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/git-projects/personal/github.com/elasticsearch_opensearch/.venv/lib/python3.8/site-packages/transformers/modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/home/ubuntu/git-projects/personal/github.com/elasticsearch_opensearch/.venv/lib/python3.8/site-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"openai/clip-vit-base-patch32\" #See https://huggingface.co/models for other options\n",
    "text_to_encode = \"example search query\" #See https://huggingface.co/docs/transformers/torchscript for more info on dummy inputs\n",
    "\n",
    "# Instantiate CLIPTextModel and CLIPProcessor with pretrained weights\n",
    "model = CLIPTextModel.from_pretrained(model_name, torchscript=True, return_dict=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Use processor to generate tensors and create dummy input\n",
    "text_inputs = processor(text=text_to_encode, return_tensors=\"pt\",max_length=77, padding=\"max_length\", truncation=True)\n",
    "dummy_input = [text_inputs['input_ids'], text_inputs['attention_mask']]\n",
    "\n",
    "# Trace model and convert to torchscript object\n",
    "traced_model = torch.jit.trace(model, dummy_input)\n",
    "\n",
    "# Save model in portable format\n",
    "torch.jit.save(traced_model, \"clip-vit-base-patch32.pt\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")\n",
    "\n",
    "# Save the model configuration\n",
    "model.config.save_pretrained(\"model_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'clip-vit-base-patch32', 'version': '1.0.0', 'model_format': 'TORCH_SCRIPT', 'model_config': {'embedding_dimension': 512, 'model_type': 'clip', 'framework_type': 'huggingface_transformers', 'pooling_mode': 'MEAN', 'normalize_result': 'true'}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# read model configuration from model_config/config.json file\n",
    "with open('model_config/config.json') as f:\n",
    "    all_config = json.load(f)\n",
    "\n",
    "mlcommons_model_config = {}\n",
    "mlcommons_model_config['name'] = \"clip-vit-base-patch32\"\n",
    "mlcommons_model_config['version'] = '1.0.0'\n",
    "mlcommons_model_config['model_format'] = 'TORCH_SCRIPT'\n",
    "\n",
    "model_config = {}\n",
    "model_config['embedding_dimension'] = 512\n",
    "model_config['model_type'] = 'clip'\n",
    "model_config['framework_type'] = 'huggingface_transformers'\n",
    "model_config['pooling_mode'] = 'MEAN'\n",
    "model_config['normalize_result'] = 'true'\n",
    "# model_config['all_config'] = json.dumps(all_config)\n",
    "\n",
    "mlcommons_model_config['model_config'] = model_config\n",
    "\n",
    "# save mlcommons model configuration to mlcommons_model_config.json file\n",
    "with open('mlcommons_model_config.json', 'w') as f:\n",
    "    json.dump(mlcommons_model_config, f)\n",
    "\n",
    "print(mlcommons_model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks 19\n",
      "Sha1 value of the model file:  14f97536887d5ced13e15564531a07a8176f528db4ece1984af5bade720ca240\n",
      "Model meta data was created successfully. Model Id:  VpoCWZIBg2jaWXNprLTn\n",
      "uploading chunk 1 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 2 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 3 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 4 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 5 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 6 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 7 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 8 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 9 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 10 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 11 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 12 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 13 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 14 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 15 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 16 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 17 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 18 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 19 of 19\n",
      "Model id: {'status': 'Uploaded'}\n",
      "Model registered successfully\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_246370/3901531281.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_config_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mlcommons_model_config.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_id_file_system\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misVerbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeploy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git-projects/personal/github.com/elasticsearch_opensearch/.venv/lib/python3.8/site-packages/opensearch_py_ml/ml_commons/ml_commons_client.py\u001b[0m in \u001b[0;36mregister_model\u001b[0;34m(self, model_path, model_config_path, model_group_id, isVerbose, deploy_model, wait_until_deployed)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# loading the model chunks from model index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeploy_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_until_deployed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_until_deployed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git-projects/personal/github.com/elasticsearch_opensearch/.venv/lib/python3.8/site-packages/opensearch_py_ml/ml_commons/ml_commons_client.py\u001b[0m in \u001b[0;36mdeploy_model\u001b[0;34m(self, model_id, wait_until_deployed)\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmodel_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"DEPLOYED\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PARTIALLY_DEPLOYED\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;31m# TODO: need to add the test case later for this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_path = \"clip-vit-base-patch32.zip\"\n",
    "model_config_path = \"mlcommons_model_config.json\"\n",
    "\n",
    "model_id_file_system = ml_client.register_model(model_path, model_config_path, isVerbose=True, deploy_model = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    # Open the image\n",
    "    with Image.open(image_path) as img:\n",
    "        # Convert image to RGB if it's not\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        \n",
    "        # Create a byte stream\n",
    "        buffered = io.BytesIO()\n",
    "        # Save the image to the byte stream in JPEG format\n",
    "        img.save(buffered, format=\"JPEG\")\n",
    "        # Encode the byte stream to base64\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    \n",
    "    return img_str\n",
    "\n",
    "# Now you can use image_binary in your OpenSearch document\n",
    "document1 = {\n",
    "    \"image_description\": \"bear holding toilet paper\",\n",
    "    \"image_binary\": image_to_base64(\"test.png\")\n",
    "}\n",
    "\n",
    "document2 = {\n",
    "    \"image_description\": \"different diaper brands\",\n",
    "    \"image_binary\": image_to_base64(\"diapers.png\")\n",
    "}\n",
    "\n",
    "# Index the document in OpenSearch\n",
    "client.index(index=\"my-nlp-index-1\", body=document1)\n",
    "client.index(index=\"my-nlp-index-1\", body=document2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
