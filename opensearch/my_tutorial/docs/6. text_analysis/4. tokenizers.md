# Tokenizers
A tokenizer receives a stream of characters and splits the text into individual tokens. A token consists of a term (usually, a word) and metadata about this term.

The output of a tokenizer is a stream of tokens. Tokenizers also maintain the following metadata about tokens:

- **The order or position of each token**: This information is used for word and phrase proximity queries.
- **The starting and ending positions (offsets) of the tokens in the text**: This information is used for highlighting search terms.
- **The token type**: Some tokenizers (for example, standard) classify tokens by type, for example, `<ALPHANUM>` or `<NUM>`. Simpler tokenizers (for example, letter) only classify tokens as type word.

## Built-in tokenizers

## Word tokenizers

Word tokenizers parse full text into words.

| Tokenizer       | Description                                                                 | Example                                                                                       |
|-----------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|
| **standard**    | - Parses strings into tokens at word boundaries<br>- Removes most punctuation | `It’s fun to contribute a brand-new PR or 2 to OpenSearch!`<br>becomes<br>`[It’s, fun, to, contribute, a, brand, new, PR, or, 2, to, OpenSearch]` |
| **letter**      | - Parses strings into tokens on any non-letter character<br>- Removes non-letter characters | `It’s fun to contribute a brand-new PR or 2 to OpenSearch!`<br>becomes<br>`[It, s, fun, to, contribute, a, brand, new, PR, or, to, OpenSearch]` |
| **lowercase**   | - Parses strings into tokens on any non-letter character<br>- Removes non-letter characters<br>- Converts terms to lowercase | `It’s fun to contribute a brand-new PR or 2 to OpenSearch!`<br>becomes<br>`[it, s, fun, to, contribute, a, brand, new, pr, or, to, opensearch]` |
| **whitespace**  | - Parses strings into tokens at white space characters | `It’s fun to contribute a brand-new PR or 2 to OpenSearch!`<br>becomes<br>`[It’s, fun, to, contribute, a, brand-new, PR, or, 2, to, OpenSearch!]` |
| **uax_url_email** | - Similar to the standard tokenizer<br>- Unlike the standard tokenizer, leaves URLs and email addresses as single terms | `It’s fun to contribute a brand-new PR or 2 to OpenSearch opensearch-project@github.com!`<br>becomes<br>`[It’s, fun, to, contribute, a, brand, new, PR, or, 2, to, OpenSearch, opensearch-project@github.com]` |
| **classic**     | - Parses strings into tokens on:<br>  - Punctuation characters that are followed by a white space character<br>  - Hyphens if the term does not contain numbers<br>- Removes punctuation<br>- Leaves URLs and email addresses as single terms | `Part number PA-35234, single-use product (128.32)`<br>becomes<br>`[Part, number, PA-35234, single, use, product, 128.32]` |
| **thai**        | - Parses Thai text into terms | `สวัสดีและยินดีต`<br>becomes<br>`[สวัสด, และ, ยินดี, ต]` |

## Partial word tokenizers

Partial word tokenizers parse text into words and generate fragments of those words for partial word matching.

| Tokenizer       | Description                                                                 | Example                                                                                       |
|-----------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|
| **ngram**       | - Parses strings into words on specified characters (for example, punctuation or white space characters) and generates n-grams of each word | `My repo`<br>becomes<br>`[M, My, y, y ,  ,  r, r, re, e, ep, p, po, o]`<br>because the default n-gram length is 1–2 characters |
| **edge_ngram**  | - Parses strings into words on specified characters (for example, punctuation or white space characters) and generates edge n-grams of each word (n-grams that start at the beginning of the word) | `My repo`<br>becomes<br>`[M, My]`<br>because the default n-gram length is 1–2 characters |


## Structured Tokenizers
[Structured Tokenizers](https://opensearch.org/docs/latest/analyzers/tokenizers/index/#structured-text-tokenizers)