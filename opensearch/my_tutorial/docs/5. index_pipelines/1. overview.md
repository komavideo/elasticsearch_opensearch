# Overview

An ingest pipeline is a sequence of processors applied to documents during ingestion into an index. Each processor performs specific tasks like filtering, transforming, or enriching data.

## Key Points

- Processors run sequentially in the order specified in the request body.
- Modified documents appear in the index after all processors are applied.
- OpenSearch ingest pipelines run within the OpenSearch cluster.

## OpenSearch Ingest Pipelines vs Data Prepper

- **OpenSearch ingest pipelines**: Preferred for simple datasets, ML processors, and vector embedding.
- **Data Prepper**: Recommended for large datasets and complex data pre-processing.

## Prerequisites

- Production environments should have at least one node with ingest role.
- With Security plugin enabled, `cluster_manage_pipelines` permission is required.

## Pipeline Definition

A pipeline definition in JSON format consists of:

```json
{
    "description" : "...",
    "processors" : [...]
}
```

# More
- https://opensearch.org/docs/latest/ingest-pipelines/