# Text analysis
# Overview

The objective of text analysis is to split the unstructured free text content of the source document into a sequence of terms, which are then stored in an inverted index. Subsequently, when a similar text analysis is applied to a user’s query, the resulting sequence of terms facilitates the matching of relevant source documents.

## Analyzers

In OpenSearch, the abstraction that encompasses text analysis is referred to as an analyzer. Each analyzer contains the following sequentially applied components:

### Character Filters

First, a character filter receives the original text as a stream of characters and adds, removes, or modifies characters in the text. For example, a character filter can strip HTML characters from a string so that the text `<p><b>Actions</b> speak louder than <em>words</em></p>` becomes `\nActions speak louder than words\n`. The output of a character filter is a stream of characters.

### Tokenizer

Next, a tokenizer receives the stream of characters that has been processed by the character filter and splits the text into individual tokens (usually, words). For example, a tokenizer can split text on white space so that the preceding text becomes `[Actions, speak, louder, than, words]`. Tokenizers also maintain metadata about tokens, such as their starting and ending positions in the text. The output of a tokenizer is a stream of tokens.

### Token Filters

Last, a token filter receives the stream of tokens from the tokenizer and adds, removes, or modifies tokens. For example, a token filter may lowercase the tokens so that `Actions` becomes `action`, remove stopwords like `than`, or add synonyms like `talk` for the word `speak`.

## Built-in Analyzers

The following table lists the built-in analyzers that OpenSearch provides. The last column of the table contains the result of applying the analyzer to the string `It’s fun to contribute a brand-new PR or 2 to OpenSearch!`.

| Analyzer            | Analysis performed                                                                 | Analyzer output                                                                 |
|---------------------|-------------------------------------------------------------------------------------|---------------------------------------------------------------------------------|
| Standard (default)  | - Parses strings into tokens at word boundaries<br>- Removes most punctuation<br>- Converts tokens to lowercase | `[it’s, fun, to, contribute, a, brand, new, pr, or, 2, to, opensearch]`         |
| Simple              | - Parses strings into tokens on any non-letter character<br>- Removes non-letter characters<br>- Converts tokens to lowercase | `[it, s, fun, to, contribute, a, brand, new, pr, or, to, opensearch]`           |
| Whitespace          | - Parses strings into tokens on white space                                         | `[It’s, fun, to, contribute, a, brand-new, PR, or, 2, to, OpenSearch!]`          |
| Stop                | - Parses strings into tokens on any non-letter character<br>- Removes non-letter characters<br>- Removes stop words<br>- Converts tokens to lowercase | `[s, fun, contribute, brand, new, pr, opensearch]`                              |
| Keyword (no-op)     | - Outputs the entire string unchanged                                               | `[It’s fun to contribute a brand-new PR or 2 to OpenSearch!]`                    |
| Pattern             | - Parses strings into tokens using regular expressions<br>- Supports converting strings to lowercase<br>- Supports removing stop words | `[it, s, fun, to, contribute, a, brand, new, pr, or, 2, to, opensearch]`         |
| Language            | Performs analysis specific to a certain language (for example, English).            | `[fun, contribut, brand, new, pr, 2, opensearch]`                               |
| Fingerprint         | - Parses strings on any non-letter character<br>- Normalizes characters by converting them to ASCII<br>- Converts tokens to lowercase<br>- Sorts, deduplicates, and concatenates tokens into a single token<br>- Supports removing stop words | `[2 a brand contribute fun it's new opensearch or pr to]`                        |

## Popular Use Cases

Some popular use cases are as below:

| Analyzer Type                | Use Cases                                                                                     |
|------------------------------|-----------------------------------------------------------------------------------------------|
| Standard Analyzer            | 1. General-purpose text analysis for most languages<br>2. Indexing and searching blog posts or articles |
| Simple Analyzer              | 1. Analyzing user-generated content where punctuation is less important<br>2. Quick analysis of informal text data like social media posts |
| Whitespace Analyzer          | 1. Analyzing log files with space-separated fields<br>2. Processing structured data where words are clearly delimited by spaces |
| Stop Analyzer                | 1. Analyzing formal documents where common words are less important<br>2. Improving search relevance by removing frequently occurring words |
| Keyword Analyzer             | 1. Indexing exact-match fields like product codes or IDs<br>2. Preserving email addresses or URLs as single tokens |
| Pattern Analyzer             | 1. Analyzing custom-formatted log files<br>2. Tokenizing text based on specific punctuation patterns |
| Language Analyzers (e.g., English, French, etc.) | 1. Analyzing content in specific languages for better search results<br>2. Implementing multi-language search functionality |
| Fingerprint Analyzer         | 1. Detecting duplicate content or plagiarism<br>2. Generating consistent keys for grouping similar text |
| Custom Analyzer              | 1. Handling domain-specific terminology or jargon<br>2. Implementing complex text analysis requirements not covered by built-in analyzers |


# Custom analyzers
If needed, you can combine tokenizers, token filters, and character filters to create a custom analyzer.

# Text analysis at indexing time and query time
OpenSearch performs text analysis on text fields when you index a document and when you send a search request

# Testing an analyzer
To test a built-in analyzer and view the list of tokens it generates when a document is indexed, you can use the Analyze API.

```json
GET /_analyze
{
  "analyzer" : "standard",
  "text" : "Let’s contribute to OpenSearch!"
}

GET /_analyze
{
  "analyzer" : "english",
  "text" : "Let’s contribute to OpenSearch!"
}

GET /_analyze
{
  "analyzer" : "simple",
  "text" : "Let’s contribute to OpenSearch!"
}
```

# Verifying analyzer settings
To verify which analyzer is associated with which field, you can use the get mapping API operation:

```
GET /testindex/_mapping
```

> if not specified, `standard` analyzer is being used

Normalizers
Tokenization divides text into individual terms, but it does not address variations in token forms. Normalization resolves these issues by converting tokens into a standard format. This ensures that similar terms are matched appropriately, even if they are not identical.

## Normalization Techniques

The following normalization techniques can help address variations in token forms:

### Case Normalization

Converts all tokens to lowercase to ensure case-insensitive matching. For example, “Hello” is normalized to “hello”.

### Stemming

Reduces words to their root form. For instance, “cars” is stemmed to “car”, and “running” is normalized to “run”.

### Synonym Handling

Treats synonyms as equivalent. For example, “jogging” and “running” can be indexed under a common term, such as “run”.


