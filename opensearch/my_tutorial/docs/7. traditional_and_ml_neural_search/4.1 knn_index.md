# k-NN index
The k-NN plugin introduces a custom data type, the knn_vector, that allows users to ingest their k-NN vectors into an OpenSearch index and perform different kinds of k-NN search. The knn_vector field is highly configurable and can serve many different k-NN workloads.For more information, see [k-NN vector](https://opensearch.org/docs/latest/field-types/supported-field-types/knn-vector/)

```json
PUT /test-index
{
  "settings": {
    "index": {
      "knn": true
    }
  },
  "mappings": {
    "properties": {
      "my_vector1": {
        "type": "knn_vector",
        "dimension": 3,
        "space_type": "l2",
        "method": {
          "name": "hnsw",
          "engine": "lucene",
          "parameters": {
            "ef_construction": 128,
            "m": 24
          }
        }
      }
    }
  }
}
```

## Byte vectors
Starting with k-NN plugin version 2.17, you can use byte vectors with the faiss and lucene engines to reduce the amount of required memory and storage space. For more information, see [Byte vectors](https://opensearch.org/docs/latest/field-types/supported-field-types/knn-vector#byte-vectors).

## Binary vectors
Starting with k-NN plugin version 2.16, you can use binary vectors with the faiss engine to reduce the amount of required storage space. For more information, see [Binary vectors](https://opensearch.org/docs/latest/field-types/supported-field-types/knn-vector#binary-vectors).

## Hardware choices
- [SIMD optimization for the Faiss engine](https://opensearch.org/docs/latest/search-plugins/knn/knn-index/#simd-optimization-for-the-faiss-engine)
- [x64 architecture](https://opensearch.org/docs/latest/search-plugins/knn/knn-index/#x64-architecture)
- [ARM64 architecture](https://opensearch.org/docs/latest/search-plugins/knn/knn-index/#arm64-architecture)

## Method aka. knn vector configuration
A method definition will always contain the name of the method, the space_type the method is built for, the engine (the library) to use, and a map of parameters.

| Mapping parameter | Required | Default | Updatable | Description |
|-------------------|----------|---------|-----------|-------------|
| name              | true     | n/a     | false     | The identifier for the nearest neighbor method. |
| space_type        | false    | l2      | false     | The vector space used to calculate the distance between vectors. Note: This value can also be specified at the top level of the mapping. |
| engine            | false    | nmslib  | false     | The approximate k-NN library to use for indexing and search. The available libraries are faiss, nmslib, and Lucene. |
| parameters        | false    | null    | false     | The parameters used for the nearest neighbor method. |

- Each engine again has [many parameters](https://opensearch.org/docs/latest/search-plugins/knn/knn-index/#method-definitions) to choose from 

# Choosing the right method
To determine the correct methods and parameters, you should first understand the requirements of your workload and what trade-offs you are willing to make. Factors to consider are 
- (1) query latency, 
- (2) query quality, 
- (3) memory limits, and 
- (4) indexing latency.  

- **If memory is not a concern, HNSW offers a strong query latency/query quality trade-off**
- **If you want to use less memory and increase indexing speed as compared to HNSW while maintaining similar query quality, you should evaluate IVF**
- **If memory is a concern, consider adding a PQ encoder to your HNSW or IVF index. Because PQ is a lossy encoding, query quality will drop**
- You can reduce the memory footprint by a factor of 2, with a minimal loss in search quality, by using the fp_16 encoder. If your vector dimensions are within the [-128, 127] byte range, we recommend using the byte quantizer to reduce the memory footprint by a factor of 4. To learn more about vector quantization options, see k-NN vector quantization.

# Memory Estimation
In a typical OpenSearch cluster, a certain portion of RAM is reserved for the JVM heap. The k-NN plugin allocates native library indexes to a portion of the remaining RAM. This portionâ€™s size is determined by the circuit_breaker_limit cluster setting. By default, the limit is set to 50%

> Having a replica doubles the total number of vectors.
> For information about using memory estimation with vector quantization, see the [vector quantization documentation](https://opensearch.org/docs/latest/search-plugins/knn/knn-vector-quantization/#memory-estimation).

## HNSW memory estimation
The memory required for HNSW is estimated to be 1.1 * (4 * dimension + 8 * M) bytes/vector.

As an example, assume you have a million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:

`1.1 * (4 * 256 + 8 * 16) * 1,000,000 ~= 1.267 GB`

## IVF memory estimation
The memory required for IVF is estimated to be 1.1 * (((4 * dimension) * num_vectors) + (4 * nlist * d)) bytes.

As an example, assume you have a million vectors with a dimension of 256 and nlist of 128. The memory requirement can be estimated as follows:

`1.1 * (((4 * 256) * 1,000,000) + (4 * 128 * 256))  ~= 1.126 GB`


# Gotchas
- Method definitions are used when the underlying approximate k-NN algorithm does not require training
- Model IDs are used when the underlying Approximate k-NN algorithm requires a training step. As a prerequisite, the model must be created with the Train API. The model contains the information needed to initialize the native library segment files


