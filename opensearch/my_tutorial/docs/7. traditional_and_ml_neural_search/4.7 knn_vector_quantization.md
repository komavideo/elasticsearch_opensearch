# Advanced 
## k-NN vector quantization
By default, the k-NN plugin supports the indexing and querying of vectors of type float, where each dimension of the vector occupies 4 bytes of memory. For use cases that require ingestion on a large scale, keeping float vectors can be expensive because OpenSearch needs to construct, load, save, and search graphs (for native nmslib and faiss engines). To reduce the memory footprint, you can use vector quantization.
OpenSearch supports many varieties of quantization. In general, the level of quantization will provide a trade-off between the accuracy of the nearest neighbor search and the size of the memory footprint consumed by the vector search. The supported types include byte vectors, **16-bit scalar quantization**, **product quantization (PQ)**, and **binary quantization(BQ)**.

[Read more](https://opensearch.org/docs/latest/search-plugins/knn/knn-vector-quantization/)

## Radial Search
Radial search enhances the k-NN pluginâ€™s capabilities beyond approximate top-k searches. With radial search, you can search all points within a vector space that reside within a specified maximum distance or minimum score threshold from a query point. This provides increased flexibility and utility in search operations.

[Read more](https://opensearch.org/docs/latest/search-plugins/knn/radial-search-knn/)

## k-NN plugin API
[Read more](https://opensearch.org/docs/latest/search-plugins/knn/api/)

## JNI libraries
[Read more](https://opensearch.org/docs/latest/search-plugins/knn/jni-libraries/)

## k-NN Settings
[Read more](https://opensearch.org/docs/latest/search-plugins/knn/settings/)

## Performance tuning
[Read more](https://opensearch.org/docs/latest/search-plugins/knn/performance-tuning/)